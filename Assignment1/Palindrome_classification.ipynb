{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjHP56QCM3xD"
      },
      "source": [
        "## 10 bit Palindrome Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "389Q0uz6I00o"
      },
      "source": [
        "### Balancing Palindrome Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyrIxdr5YTEQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/palindrome_data.csv\")\n",
        "\n",
        "# Separate palindrome and non-palindrome examples\n",
        "np.random.seed(seed_value)  # Set seed for consistency\n",
        "palindrome_examples = df[df['y'] == 1]\n",
        "non_palindrome_examples = df[df['y'] == 0]\n",
        "\n",
        "# Oversample palindrome examples to match the number of non-palindrome examples\n",
        "oversampled_palindrome = palindrome_examples.sample(n=len(non_palindrome_examples), replace=True, random_state=seed_value)\n",
        "\n",
        "# Concatenate oversampled palindrome examples with non-palindrome examples\n",
        "balanced_df = pd.concat([oversampled_palindrome, non_palindrome_examples])\n",
        "\n",
        "# Shuffle the dataset\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=seed_value).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO9W1XH8i7kH"
      },
      "source": [
        "### Train-Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlBKCO5hjAWd",
        "outputId": "b6da764b-4f50-4813-e044-627291bcc383"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1588, 10), (1588, 1), (396, 10), (396, 1))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define train-test split function\n",
        "def train_test_split(data, test_size=0.2):\n",
        "    num_samples = len(data)\n",
        "    num_test_samples = int(test_size * num_samples)\n",
        "\n",
        "    # Shuffle the data\n",
        "    shuffled_indices = np.random.permutation(num_samples)\n",
        "    data = data.iloc[shuffled_indices]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    test_data = data[:num_test_samples]\n",
        "    train_data = data[num_test_samples:]\n",
        "\n",
        "    return train_data, test_data\n",
        "\n",
        "# Perform train-test split\n",
        "train_data, test_data = train_test_split(balanced_df, test_size=0.2)\n",
        "\n",
        "X_train = train_data.iloc[:, :balanced_df.shape[1] - 1].to_numpy()\n",
        "y_train = train_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "X_test = test_data.iloc[:, :balanced_df.shape[1] - 1].to_numpy()\n",
        "y_test = test_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT9sUpw1JFIc"
      },
      "source": [
        "### Neural Network Training with Momentum for Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqp9YVXvOY-b"
      },
      "outputs": [],
      "source": [
        "# Define sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define derivative of sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Define binary cross-entropy loss function\n",
        "def binary_crossentropy(y_true, y_pred):\n",
        "    epsilon = 1e-7  # Small constant to prevent numerical instability\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predicted values to avoid extreme values\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "\n",
        "# Define function to initialize weights and biases\n",
        "def initialize_parameters(input_size, hidden_size):\n",
        "    W1 = np.random.randn(input_size, hidden_size)\n",
        "    b1 = np.zeros((1, hidden_size))\n",
        "    W2 = np.random.randn(hidden_size, 1)\n",
        "    b2 = np.zeros((1, 1))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "# Define function for forward propagation\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "# Define function for backpropagation\n",
        "def backward_propagation(X, y, Z1, A1, Z2, A2, W2):\n",
        "    m = len(X)\n",
        "    dZ2 = A2 - y\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "    dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(A1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Define function for training the model\n",
        "def train(X_train, y_train, hidden_size, learning_rate, momentum_rate, epochs):\n",
        "    input_size = X_train.shape[1]\n",
        "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size)\n",
        "\n",
        "    # Initialize velocities for momentum update\n",
        "    v_dW1 = np.zeros_like(W1)\n",
        "    v_db1 = np.zeros_like(b1)\n",
        "    v_dW2 = np.zeros_like(W2)\n",
        "    v_db2 = np.zeros_like(b2)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward propagation\n",
        "        Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = binary_crossentropy(y_train, A2)\n",
        "\n",
        "        # Backward propagation\n",
        "        dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, Z1, A1, Z2, A2, W2)\n",
        "\n",
        "        # Update velocities\n",
        "        v_dW1 = momentum_rate * v_dW1 + learning_rate * dW1\n",
        "        v_db1 = momentum_rate * v_db1 + learning_rate * db1\n",
        "        v_dW2 = momentum_rate * v_dW2 + learning_rate * dW2\n",
        "        v_db2 = momentum_rate * v_db2 + learning_rate * db2\n",
        "\n",
        "        # Update weights and biases with momentum\n",
        "        W1 -= v_dW1\n",
        "        b1 -= v_db1\n",
        "        W2 -= v_dW2\n",
        "        b2 -= v_db2\n",
        "\n",
        "        if epoch % 10000 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Define function for predicting\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
        "    return np.round(A2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC5kTGBnJaeB"
      },
      "source": [
        "### Train and validate using 4-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR0pRKXZczPX",
        "outputId": "4d118c23-1730-49c2-f212-dc3ac7bda172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.7377406072154736\n",
            "Epoch 10000, Loss: 0.18209339868905045\n",
            "Epoch 20000, Loss: 0.07613950676205246\n",
            "Epoch 30000, Loss: 0.050998251581506907\n",
            "Epoch 40000, Loss: 0.03801857841236874\n",
            "Epoch 50000, Loss: 0.029536136649982225\n",
            "Epoch 60000, Loss: 0.022464758622468935\n",
            "Epoch 70000, Loss: 0.016824047284219764\n",
            "Epoch 80000, Loss: 0.012735636500527253\n",
            "Epoch 90000, Loss: 0.009877090278697915\n",
            "Epoch 100000, Loss: 0.007866868349177492\n",
            "Epoch 110000, Loss: 0.006422666729759444\n",
            "Epoch 120000, Loss: 0.005358157602882435\n",
            "Epoch 130000, Loss: 0.0045534013397981864\n",
            "Epoch 140000, Loss: 0.003930675877780064\n",
            "Epoch 0, Loss: 0.7115950705228873\n",
            "Epoch 10000, Loss: 0.1369416906120831\n",
            "Epoch 20000, Loss: 0.07245417603703959\n",
            "Epoch 30000, Loss: 0.047389481875396086\n",
            "Epoch 40000, Loss: 0.036510497761720696\n",
            "Epoch 50000, Loss: 0.03024384874968493\n",
            "Epoch 60000, Loss: 0.026298394392142325\n",
            "Epoch 70000, Loss: 0.023566358963272034\n",
            "Epoch 80000, Loss: 0.02139856448023795\n",
            "Epoch 90000, Loss: 0.01935392006528496\n",
            "Epoch 100000, Loss: 0.017229901421798876\n",
            "Epoch 110000, Loss: 0.015086447922386108\n",
            "Epoch 120000, Loss: 0.013067751445075588\n",
            "Epoch 130000, Loss: 0.011272551298177627\n",
            "Epoch 140000, Loss: 0.009733315114449036\n",
            "Epoch 0, Loss: 0.6930632770928595\n",
            "Epoch 10000, Loss: 0.134866075197439\n",
            "Epoch 20000, Loss: 0.08336168414547146\n",
            "Epoch 30000, Loss: 0.06038064781605745\n",
            "Epoch 40000, Loss: 0.0486824204219783\n",
            "Epoch 50000, Loss: 0.03968066158203177\n",
            "Epoch 60000, Loss: 0.03127830424458794\n",
            "Epoch 70000, Loss: 0.02428924788122269\n",
            "Epoch 80000, Loss: 0.018761988492079825\n",
            "Epoch 90000, Loss: 0.014612507317476866\n",
            "Epoch 100000, Loss: 0.011571263773109007\n",
            "Epoch 110000, Loss: 0.009345705295662657\n",
            "Epoch 120000, Loss: 0.007698443744204534\n",
            "Epoch 130000, Loss: 0.006457995212661997\n",
            "Epoch 140000, Loss: 0.005505899198977739\n",
            "Epoch 0, Loss: 0.7263719001643529\n",
            "Epoch 10000, Loss: 0.5917415321870176\n",
            "Epoch 20000, Loss: 0.5575735851985177\n",
            "Epoch 30000, Loss: 0.5471503928557585\n",
            "Epoch 40000, Loss: 0.5415474675271216\n",
            "Epoch 50000, Loss: 0.5380285243411635\n",
            "Epoch 60000, Loss: 0.5356043482784759\n",
            "Epoch 70000, Loss: 0.5338020186868783\n",
            "Epoch 80000, Loss: 0.5323686535509501\n",
            "Epoch 90000, Loss: 0.5311677456539692\n",
            "Epoch 100000, Loss: 0.5301276343568686\n",
            "Epoch 110000, Loss: 0.5292142000615638\n",
            "Epoch 120000, Loss: 0.5284081570505461\n",
            "Epoch 130000, Loss: 0.5276928181517428\n",
            "Epoch 140000, Loss: 0.527053810841881\n",
            "\n",
            "Accuracy scores: [0.9848866498740554, 0.9798488664987406, 0.9924433249370277, 0.7455919395465995]\n",
            "Precision scores: [0.9711538461538461, 0.9587628865979382, 0.9850746268656716, 0.7384615384615385]\n",
            "Recall scores: [1.0, 1.0, 1.0, 0.7422680412371134]\n",
            "F1 scores: [0.9853658536585366, 0.9789473684210526, 0.9924812030075187, 0.7403598971722365]\n",
            "\n",
            "Best Weights: {'W1': array([[ -3.31356105,   3.05946653],\n",
            "       [ -6.79424271,   6.47104638],\n",
            "       [-27.36603563,  25.95887163],\n",
            "       [-13.67974937,  12.9207158 ],\n",
            "       [ -8.5507808 ,   8.00924326],\n",
            "       [  8.53514438,  -7.99122157],\n",
            "       [ 13.71802007, -13.0204706 ],\n",
            "       [ 27.31115992, -25.88953623],\n",
            "       [  6.78209314,  -6.48160892],\n",
            "       [  3.53770451,  -3.51028444]]), 'b1': array([[-1.73611547, -1.46604695]]), 'W2': array([[-33.93097329],\n",
            "       [-33.40431715]]), 'b2': array([[16.14300463]])}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.2\n",
        "momentum_rate = 0.09\n",
        "epochs = 150000\n",
        "hidden_size = 2\n",
        "\n",
        "# Perform 4-fold cross-validation\n",
        "k = 4\n",
        "fold_size = len(X_train) // k\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "best_weights = None\n",
        "\n",
        "for fold in range(k):\n",
        "    # Split data into train and val sets\n",
        "    X_fold_train = np.concatenate((X_train[:fold * fold_size], X_train[(fold + 1) * fold_size:]), axis=0)\n",
        "    y_fold_train = np.concatenate((y_train[:fold * fold_size], y_train[(fold + 1) * fold_size:]), axis=0)\n",
        "    X_fold_val = X_train[fold * fold_size:(fold + 1) * fold_size]\n",
        "    y_fold_val = y_train[fold * fold_size:(fold + 1) * fold_size]\n",
        "\n",
        "    # Train the model\n",
        "    W1, b1, W2, b2 = train(X_fold_train, y_fold_train, hidden_size, learning_rate, momentum_rate, epochs)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = predict(X_fold_val, W1, b1, W2, b2)\n",
        "\n",
        "    # Calculate TP, TN, FP, FN\n",
        "    TP = np.sum((y_pred == 1) & (y_fold_val == 1))\n",
        "    TN = np.sum((y_pred == 0) & (y_fold_val == 0))\n",
        "    FP = np.sum((y_pred == 1) & (y_fold_val == 0))\n",
        "    FN = np.sum((y_pred == 0) & (y_fold_val == 1))\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (TP + TN) / len(y_fold_val)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    # Precision\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    precision_scores.append(precision)\n",
        "\n",
        "    # Recall\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    recall_scores.append(recall)\n",
        "\n",
        "    # F1-score\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Keep track of best weights and biases\n",
        "    if best_weights is None or f1 == np.max(f1_scores):\n",
        "        best_weights = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "\n",
        "# cross validation performance metrics\n",
        "print(\"\\nAccuracy scores:\", accuracy_scores)\n",
        "print(\"Precision scores:\", precision_scores)\n",
        "print(\"Recall scores:\", recall_scores)\n",
        "print(\"F1 scores:\", f1_scores)\n",
        "print(\"\\nBest Weights:\", best_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWxX4AcFnutr"
      },
      "source": [
        "## Predictions on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ5h0nE_njl5",
        "outputId": "a9ccefd7-650d-466b-ac55-c9b61c27551a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Overall Performance on Test Data ===\n",
            "Total Accuracy: 0.98989898989899\n",
            "Class 0 (Non-Palindrome) Accuracy: 0.9782608695652174\n",
            "Class 1 (Palindrome) Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Use best weights for predictions on test data\n",
        "W1 = best_weights['W1']\n",
        "b1 = best_weights['b1']\n",
        "W2 = best_weights['W2']\n",
        "b2 = best_weights['b2']\n",
        "\n",
        "y_pred_test = predict(X_test, W1, b1, W2, b2)\n",
        "\n",
        "# Calculate performance metrics on test data\n",
        "TP_test = np.sum((y_pred_test == 1) & (y_test == 1))\n",
        "TN_test = np.sum((y_pred_test == 0) & (y_test == 0))\n",
        "FP_test = np.sum((y_pred_test == 1) & (y_test == 0))\n",
        "FN_test = np.sum((y_pred_test == 0) & (y_test == 1))\n",
        "\n",
        "# Overall accuracy on test data\n",
        "total_accuracy_test = (TP_test + TN_test) / len(y_test)\n",
        "\n",
        "# Accuracy for class 0 (Non-Palindrome) on test data\n",
        "class_0_accuracy_test = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else 0\n",
        "\n",
        "# Accuracy for class 1 (Palindrome) on test data\n",
        "class_1_accuracy_test = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else 0\n",
        "\n",
        "# Print overall performance metrics on test data\n",
        "print(\"\\n=== Overall Performance on Test Data ===\")\n",
        "print(\"Total Accuracy:\", total_accuracy_test)\n",
        "print(\"Class 0 (Non-Palindrome) Accuracy:\", class_0_accuracy_test)\n",
        "print(\"Class 1 (Palindrome) Accuracy:\", class_1_accuracy_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oo-uD9INzEs"
      },
      "source": [
        "### Code Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuwAdwp3NvaV",
        "outputId": "63eead0f-953e-4e98-dd23-a2803dca051a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[False]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "W1 = np.array([[ -3.31356105,   3.05946653],\n",
        "       [ -6.79424271,   6.47104638],\n",
        "       [-27.36603563,  25.95887163],\n",
        "       [-13.67974937,  12.9207158 ],\n",
        "       [ -8.5507808 ,   8.00924326],\n",
        "       [  8.53514438,  -7.99122157],\n",
        "       [ 13.71802007, -13.0204706 ],\n",
        "       [ 27.31115992, -25.88953623],\n",
        "       [  6.78209314,  -6.48160892],\n",
        "       [  3.53770451,  -3.51028444]])\n",
        "b1 = np.array([[-1.73611547, -1.46604695]])\n",
        "W2 = np.array([[-33.93097329],[-33.40431715]])\n",
        "b2 = np.array([[16.14300463]])\n",
        "\n",
        "demoX = np.array([[1,0,1,1,1,1,1,0,1,1]])\n",
        "Z1, A1, Z2, A2 = forward_propagation(demoX, W1, b1, W2, b2)\n",
        "print(A2 > 0.5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
