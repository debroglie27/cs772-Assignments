{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjHP56QCM3xD"
   },
   "source": [
    "# Assignment-1: 10 bit Palindrome Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Seed Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "**Palindrome:** 32  \n",
    "**Non-Palindrome:** 992\n",
    "\n",
    "**First 10 Columns:** 10-bits of the input  \n",
    "**Last Column:** Class label (0: Non-Palindrome, 1: Palindrome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"palindrome_data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "389Q0uz6I00o"
   },
   "source": [
    "### Balancing Palindrome Data\n",
    "\n",
    "**Palindrome:** 992  \n",
    "**Non-Palindrome:** 992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lyrIxdr5YTEQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1984, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate palindrome and non-palindrome examples\n",
    "np.random.seed(seed_value)  # Set seed for consistency\n",
    "palindrome_examples = df[df['y'] == 1]\n",
    "non_palindrome_examples = df[df['y'] == 0]\n",
    "\n",
    "# Oversample palindrome examples to match the number of non-palindrome examples\n",
    "oversampled_palindrome = palindrome_examples.sample(n=len(non_palindrome_examples), replace=True, random_state=seed_value)\n",
    "\n",
    "# Concatenate oversampled palindrome examples with non-palindrome examples\n",
    "balanced_df = pd.concat([oversampled_palindrome, non_palindrome_examples])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=seed_value).reset_index(drop=True)\n",
    "\n",
    "balanced_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eO9W1XH8i7kH"
   },
   "source": [
    "### Train-Test split\n",
    "\n",
    "The 4-Fold Cross Validation is applied on the training set  \n",
    "After training, weights of the fold which gives the best result is used to predict on the test set.\n",
    "\n",
    "**Training Set:** 80%  \n",
    "**Test Set:** 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlBKCO5hjAWd",
    "outputId": "b6da764b-4f50-4813-e044-627291bcc383"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1588, 10), (1588, 1), (396, 10), (396, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define train-test split function\n",
    "def train_test_split(data, test_size=0.2):\n",
    "    num_samples = len(data)\n",
    "    num_test_samples = int(test_size * num_samples)\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.seed(seed_value)\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    data = data.iloc[shuffled_indices]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    test_data = data[:num_test_samples]\n",
    "    train_data = data[num_test_samples:]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Perform train-test split\n",
    "train_data, test_data = train_test_split(balanced_df, test_size=0.2)\n",
    "\n",
    "X_train = train_data.iloc[:, :balanced_df.shape[1] - 1].to_numpy()\n",
    "y_train = train_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
    "\n",
    "X_test = test_data.iloc[:, :balanced_df.shape[1] - 1].to_numpy()\n",
    "y_test = test_data.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT9sUpw1JFIc"
   },
   "source": [
    "### Neural Network Training with Momentum for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jqp9YVXvOY-b"
   },
   "outputs": [],
   "source": [
    "# Define sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define derivative of sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define binary cross-entropy loss function\n",
    "def binary_crossentropy(y_true, y_pred):\n",
    "    epsilon = 1e-7  # Small constant to prevent numerical instability\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip predicted values to avoid extreme values\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "\n",
    "# Define function to initialize weights and biases\n",
    "def initialize_parameters(input_size, hidden_size):\n",
    "    W1 = np.random.randn(input_size, hidden_size)\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    W2 = np.random.randn(hidden_size, 1)\n",
    "    b2 = np.zeros((1, 1))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "# Define function for forward propagation\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# Define function for backpropagation\n",
    "def backward_propagation(X, y, Z1, A1, Z2, A2, W2):\n",
    "    m = len(X)\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    dZ1 = np.dot(dZ2, W2.T) * sigmoid_derivative(A1)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "# Define function for training the model\n",
    "def train(X_train, y_train, hidden_size, learning_rate, momentum_rate, epochs):\n",
    "    input_size = X_train.shape[1]\n",
    "    W1, b1, W2, b2 = initialize_parameters(input_size, hidden_size)\n",
    "\n",
    "    # Initialize velocities for momentum update\n",
    "    v_dW1 = np.zeros_like(W1)\n",
    "    v_db1 = np.zeros_like(b1)\n",
    "    v_dW2 = np.zeros_like(W2)\n",
    "    v_db2 = np.zeros_like(b2)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward propagation\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = binary_crossentropy(y_train, A2)\n",
    "\n",
    "        # Backward propagation\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, Z1, A1, Z2, A2, W2)\n",
    "\n",
    "        # Update velocities\n",
    "        v_dW1 = momentum_rate * v_dW1 + learning_rate * dW1\n",
    "        v_db1 = momentum_rate * v_db1 + learning_rate * db1\n",
    "        v_dW2 = momentum_rate * v_dW2 + learning_rate * dW2\n",
    "        v_db2 = momentum_rate * v_db2 + learning_rate * db2\n",
    "\n",
    "        # Update weights and biases with momentum\n",
    "        W1 -= v_dW1\n",
    "        b1 -= v_db1\n",
    "        W2 -= v_dW2\n",
    "        b2 -= v_db2\n",
    "\n",
    "        if epoch % 10000 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Define function for predicting\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "    return np.round(A2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XC5kTGBnJaeB"
   },
   "source": [
    "### Train and validate using 4-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jR0pRKXZczPX",
    "outputId": "4d118c23-1730-49c2-f212-dc3ac7bda172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== For Fold 1 ===\n",
      "Epoch 0, Loss: 0.732873019625102\n",
      "Epoch 10000, Loss: 0.16023590111678473\n",
      "Epoch 20000, Loss: 0.07137059810935208\n",
      "Epoch 30000, Loss: 0.04647842494961538\n",
      "Epoch 40000, Loss: 0.0350398996588649\n",
      "Epoch 50000, Loss: 0.026918800972093915\n",
      "Epoch 60000, Loss: 0.020760438058904665\n",
      "Epoch 70000, Loss: 0.01611380717466843\n",
      "Epoch 80000, Loss: 0.012631929521740339\n",
      "Epoch 90000, Loss: 0.010063772944728941\n",
      "Epoch 100000, Loss: 0.008170165615542386\n",
      "Epoch 110000, Loss: 0.006758690922960202\n",
      "Epoch 120000, Loss: 0.005689312065235438\n",
      "Epoch 130000, Loss: 0.004864317264594233\n",
      "Epoch 140000, Loss: 0.004216305783863969\n",
      "\n",
      "=== For Fold 2 ===\n",
      "Epoch 0, Loss: 0.7127233865667766\n",
      "Epoch 10000, Loss: 0.1486693071231661\n",
      "Epoch 20000, Loss: 0.07883076731131508\n",
      "Epoch 30000, Loss: 0.0555464557780053\n",
      "Epoch 40000, Loss: 0.04248299121614781\n",
      "Epoch 50000, Loss: 0.03160315061928437\n",
      "Epoch 60000, Loss: 0.02469703917900714\n",
      "Epoch 70000, Loss: 0.020427622498021176\n",
      "Epoch 80000, Loss: 0.01767117933415973\n",
      "Epoch 90000, Loss: 0.015334491449537187\n",
      "Epoch 100000, Loss: 0.012766917501468206\n",
      "Epoch 110000, Loss: 0.010575569439023672\n",
      "Epoch 120000, Loss: 0.008829302463362079\n",
      "Epoch 130000, Loss: 0.007454684657631252\n",
      "Epoch 140000, Loss: 0.006370120371389395\n",
      "\n",
      "=== For Fold 3 ===\n",
      "Epoch 0, Loss: 0.6933066430971017\n",
      "Epoch 10000, Loss: 0.13339375902579945\n",
      "Epoch 20000, Loss: 0.08421851717623637\n",
      "Epoch 30000, Loss: 0.06149626082139822\n",
      "Epoch 40000, Loss: 0.04778858384540589\n",
      "Epoch 50000, Loss: 0.04041090386584912\n",
      "Epoch 60000, Loss: 0.03179719907667397\n",
      "Epoch 70000, Loss: 0.025050590582182748\n",
      "Epoch 80000, Loss: 0.020688899292643628\n",
      "Epoch 90000, Loss: 0.017891521840927214\n",
      "Epoch 100000, Loss: 0.016040423608911305\n",
      "Epoch 110000, Loss: 0.014715836277870592\n",
      "Epoch 120000, Loss: 0.013420913424251115\n",
      "Epoch 130000, Loss: 0.012048586868843315\n",
      "Epoch 140000, Loss: 0.010802460720686871\n",
      "\n",
      "=== For Fold 4 ===\n",
      "Epoch 0, Loss: 0.7213226867774963\n",
      "Epoch 10000, Loss: 0.47012582386708796\n",
      "Epoch 20000, Loss: 0.41077093681582405\n",
      "Epoch 30000, Loss: 0.385257299344717\n",
      "Epoch 40000, Loss: 0.3739753322922049\n",
      "Epoch 50000, Loss: 0.3676519410214087\n",
      "Epoch 60000, Loss: 0.36357104046313654\n",
      "Epoch 70000, Loss: 0.3606950858023181\n",
      "Epoch 80000, Loss: 0.35854498302593796\n",
      "Epoch 90000, Loss: 0.35686233403089146\n",
      "Epoch 100000, Loss: 0.3554952190282939\n",
      "Epoch 110000, Loss: 0.354349317239551\n",
      "Epoch 120000, Loss: 0.35336136849154504\n",
      "Epoch 130000, Loss: 0.3524977662422635\n",
      "Epoch 140000, Loss: 0.3517443546295745\n",
      "\n",
      "Accuracy scores: [0.9848866498740554, 0.9874055415617129, 0.9874055415617129, 0.7329974811083123]\n",
      "Precision scores: [0.9711538461538461, 0.9738219895287958, 0.9753694581280788, 0.9313725490196079]\n",
      "Recall scores: [1.0, 1.0, 1.0, 0.4896907216494845]\n",
      "F1 scores: [0.9853658536585366, 0.986737400530504, 0.9875311720698255, 0.6418918918918919]\n",
      "\n",
      "Best Weights: {'W1': array([[ -5.76167255,   5.56514563],\n",
      "       [ -3.69716918,   3.73053448],\n",
      "       [-25.13589403,  24.81737777],\n",
      "       [-14.88346186,  14.62665054],\n",
      "       [ -7.59081964,   7.47953543],\n",
      "       [  7.75162923,  -7.55310955],\n",
      "       [ 14.75580699, -14.55440675],\n",
      "       [ 25.17350141, -24.83710604],\n",
      "       [  3.71419654,  -3.75168649],\n",
      "       [  5.20895038,  -5.05157216]]), 'b1': array([[-1.40341082, -1.82660566]]), 'W2': array([[-32.43122024],\n",
      "       [-32.6113148 ]]), 'b2': array([[15.84870193]])}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.2\n",
    "momentum_rate = 0.09\n",
    "epochs = 150000\n",
    "hidden_size = 2\n",
    "\n",
    "# Perform 4-fold cross-validation\n",
    "k = 4\n",
    "fold_size = len(X_train) // k\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "best_weights = None\n",
    "\n",
    "for fold in range(k):\n",
    "    print(f'\\n=== For Fold {fold+1} ===')\n",
    "    # Split data into train and val sets\n",
    "    X_fold_train = np.concatenate((X_train[:fold * fold_size], X_train[(fold + 1) * fold_size:]), axis=0)\n",
    "    y_fold_train = np.concatenate((y_train[:fold * fold_size], y_train[(fold + 1) * fold_size:]), axis=0)\n",
    "    X_fold_val = X_train[fold * fold_size:(fold + 1) * fold_size]\n",
    "    y_fold_val = y_train[fold * fold_size:(fold + 1) * fold_size]\n",
    "\n",
    "    # Train the model\n",
    "    W1, b1, W2, b2 = train(X_fold_train, y_fold_train, hidden_size, learning_rate, momentum_rate, epochs)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = predict(X_fold_val, W1, b1, W2, b2)\n",
    "\n",
    "    # Calculate TP, TN, FP, FN\n",
    "    TP = np.sum((y_pred == 1) & (y_fold_val == 1))\n",
    "    TN = np.sum((y_pred == 0) & (y_fold_val == 0))\n",
    "    FP = np.sum((y_pred == 1) & (y_fold_val == 0))\n",
    "    FN = np.sum((y_pred == 0) & (y_fold_val == 1))\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = (TP + TN) / len(y_fold_val)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "    # Precision\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    precision_scores.append(precision)\n",
    "\n",
    "    # Recall\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "    # F1-score\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    # Keep track of best weights and biases\n",
    "    if best_weights is None or f1 == np.max(f1_scores):\n",
    "        best_weights = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "# cross validation performance metrics\n",
    "print(\"\\nAccuracy scores:\", accuracy_scores)\n",
    "print(\"Precision scores:\", precision_scores)\n",
    "print(\"Recall scores:\", recall_scores)\n",
    "print(\"F1 scores:\", f1_scores)\n",
    "print(\"\\nBest Weights:\", best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWxX4AcFnutr"
   },
   "source": [
    "## Predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJ5h0nE_njl5",
    "outputId": "a9ccefd7-650d-466b-ac55-c9b61c27551a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Performance on Test Data ===\n",
      "Total Accuracy: 0.9974747474747475\n",
      "Class 0 (Non-Palindrome) Accuracy: 0.9945652173913043\n",
      "Class 1 (Palindrome) Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Use best weights for predictions on test data\n",
    "W1 = best_weights['W1']\n",
    "b1 = best_weights['b1']\n",
    "W2 = best_weights['W2']\n",
    "b2 = best_weights['b2']\n",
    "\n",
    "y_pred_test = predict(X_test, W1, b1, W2, b2)\n",
    "\n",
    "# Calculate performance metrics on test data\n",
    "TP_test = np.sum((y_pred_test == 1) & (y_test == 1))\n",
    "TN_test = np.sum((y_pred_test == 0) & (y_test == 0))\n",
    "FP_test = np.sum((y_pred_test == 1) & (y_test == 0))\n",
    "FN_test = np.sum((y_pred_test == 0) & (y_test == 1))\n",
    "\n",
    "# Overall accuracy on test data\n",
    "total_accuracy_test = (TP_test + TN_test) / len(y_test)\n",
    "\n",
    "# Accuracy for class 0 (Non-Palindrome) on test data\n",
    "class_0_accuracy_test = TN_test / (TN_test + FP_test) if (TN_test + FP_test) > 0 else 0\n",
    "\n",
    "# Accuracy for class 1 (Palindrome) on test data\n",
    "class_1_accuracy_test = TP_test / (TP_test + FN_test) if (TP_test + FN_test) > 0 else 0\n",
    "\n",
    "# Print overall performance metrics on test data\n",
    "print(\"\\n=== Overall Performance on Test Data ===\")\n",
    "print(\"Total Accuracy:\", total_accuracy_test)\n",
    "print(\"Class 0 (Non-Palindrome) Accuracy:\", class_0_accuracy_test)\n",
    "print(\"Class 1 (Palindrome) Accuracy:\", class_1_accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oo-uD9INzEs"
   },
   "source": [
    "## Code Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IuwAdwp3NvaV",
    "outputId": "63eead0f-953e-4e98-dd23-a2803dca051a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1 0 0 1 1 1 1 0 0 1]\n",
      "Result: True\n"
     ]
    }
   ],
   "source": [
    "# Change the array to provide your input\n",
    "demoX = np.array([[1,0,0,1,1,1,1,0,0,1]])\n",
    "\n",
    "W1 = best_weights['W1']\n",
    "b1 = best_weights['b1']\n",
    "W2 = best_weights['W2']\n",
    "b2 = best_weights['b2']\n",
    "\n",
    "_, _, _, A2 = forward_propagation(demoX, W1, b1, W2, b2)\n",
    "print(\"Input:\", demoX.squeeze())\n",
    "print(\"Result:\", (A2 > 0.5).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
